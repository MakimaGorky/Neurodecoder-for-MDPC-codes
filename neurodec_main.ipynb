{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:01.355422300Z",
     "start_time": "2025-06-03T20:15:01.343989200Z"
    },
    "id": "SkdP3GIuRXhJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:02.018471500Z",
     "start_time": "2025-06-03T20:15:02.006665900Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:03.296701700Z",
     "start_time": "2025-06-03T20:15:03.282377300Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_matrix(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            matr = torch.tensor([[int(elem) for elem in line.split()] for line in f.readlines()], device=device)\n",
    "            return matr\n",
    "    except Exception as e:\n",
    "        print('bad filename')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDPC 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:04.217138Z",
     "start_time": "2025-06-03T20:15:04.195320400Z"
    }
   },
   "outputs": [],
   "source": [
    "enc_path = 'G-45-73.txt'\n",
    "syn_path = 'H-73-73-2.txt'\n",
    "encoding_matrix = parse_matrix(enc_path)\n",
    "syndrome_matrix = parse_matrix(syn_path)\n",
    "\n",
    "print(encoding_matrix)\n",
    "print(syndrome_matrix)\n",
    "\n",
    "encoding_matrix = encoding_matrix.float()\n",
    "syndrome_matrix = syndrome_matrix.float()\n",
    "\n",
    "k, n = encoding_matrix.shape\n",
    "syndrome_len = syndrome_matrix.shape[0] \n",
    "\n",
    "input_size = n + syndrome_len # –í—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä. –°–∏–Ω–¥—Ä–æ–º + –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ\n",
    "output_size = n  # –í—ã—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä. –û—Ü–µ–Ω–∫–∞ –æ—à–∏–±–æ–∫ –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö\n",
    "hidden_size = 6 * output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDPC 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_path = 'G-53-105.txt'\n",
    "syn_path = 'H-105-105.txt'\n",
    "encoding_matrix = parse_matrix(enc_path)\n",
    "syndrome_matrix = parse_matrix(syn_path).T\n",
    "\n",
    "print(encoding_matrix)\n",
    "print(syndrome_matrix)\n",
    "\n",
    "encoding_matrix = encoding_matrix.float()\n",
    "syndrome_matrix = syndrome_matrix.float()\n",
    "\n",
    "k, n = encoding_matrix.shape\n",
    "syndrome_len = syndrome_matrix.shape[0]\n",
    "\n",
    "input_size = n + syndrome_len # –í—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä. –°–∏–Ω–¥—Ä–æ–º + –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ\n",
    "output_size = n  # –í—ã—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä. –û—Ü–µ–Ω–∫–∞ –æ—à–∏–±–æ–∫ –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö\n",
    "hidden_size = 6 * output_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJBdjg6pbBzs"
   },
   "source": [
    "# data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:08.608261700Z",
     "start_time": "2025-06-03T20:15:08.601259100Z"
    },
    "id": "_JqdDuv4Sfo2"
   },
   "outputs": [],
   "source": [
    "# n is message_length\n",
    "def gen_encoded_messages(num_mess, device):\n",
    "    res = torch.empty(num_mess, n, dtype=torch.float32, device=device)\n",
    "    for i in range(num_mess):\n",
    "\n",
    "        input_message = torch.randint(0, 2, (k,), dtype=torch.float32, device=device)\n",
    "        # encoding\n",
    "        enc_mess = (torch.matmul(input_message, encoding_matrix) % 2).float()\n",
    "        # BPSK\n",
    "        enc_mod_mess = (2 * enc_mess - 1).float()\n",
    "        \n",
    "        res[i, :] = enc_mod_mess\n",
    "    return res\n",
    "\n",
    "\n",
    "def noisify(messages, device, snr=4):\n",
    "\n",
    "    linear_snr = 10 ** (snr / 10)\n",
    "    sigma = math.sqrt(1 / (2 * k / n * linear_snr))\n",
    "    \n",
    "    noise = torch.normal(0, sigma, size=messages.shape, dtype=torch.float32, device=device)\n",
    "      \n",
    "    return messages + noise\n",
    "\n",
    "\n",
    "def get_syndromes(messages):\n",
    "    # BPSK demodulation\n",
    "    demo_mes = (messages > 0).float()\n",
    "    # returning syndrome\n",
    "    return torch.matmul(demo_mes, syndrome_matrix.T) % 2\n",
    "\n",
    "\n",
    "def get_abses(messages):\n",
    "    return torch.abs(messages)\n",
    "\n",
    "\n",
    "def get_pure_error(enc_mod_messes, noisy_meses):\n",
    "    c = noisy_meses * enc_mod_messes\n",
    "    return (c < 0).float()\n",
    "\n",
    "\n",
    "def gen_data(data_len, device=torch.device(\"cpu\"), snr=4):\n",
    "    messes = gen_encoded_messages(data_len, device)\n",
    "    noisy_messes = noisify(messes, device, snr)\n",
    "    \n",
    "    abses = get_abses(noisy_messes)\n",
    "    syns = get_syndromes(noisy_messes)\n",
    "    perrors = get_pure_error(messes, noisy_messes)\n",
    "\n",
    "    synabses = torch.cat((syns, abses), dim=1)\n",
    "    return synabses, perrors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t414QSgxllmo"
   },
   "source": [
    "# Infinite dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:09.482427800Z",
     "start_time": "2025-06-03T20:15:09.473352700Z"
    },
    "id": "NAxJGIOslkO8"
   },
   "outputs": [],
   "source": [
    "class InfiniteDataloader(Dataset):\n",
    "    def __init__(self, limit, gen_function, device, batch_size=512, shuffle=False, snr=4):\n",
    "        self.gen_function = gen_function\n",
    "        self.snr = snr\n",
    "        \n",
    "        self.batch_size = int(batch_size)\n",
    "        self.limit = int(limit)\n",
    "        self.current = 0\n",
    "        \n",
    "        self.need_shuffle = shuffle\n",
    "        self.indices = torch.arange(self.limit)\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.limit + self.batch_size -1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current = 0\n",
    "        if self.need_shuffle:\n",
    "            self.indices = torch.randperm(self.limit)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current >= self.limit:\n",
    "            raise StopIteration\n",
    "        \n",
    "        cur_batch_size = min(self.batch_size, self.limit - self.current)\n",
    "        synabses, pure_errors = self.gen_function(cur_batch_size, self.device, snr=self.snr)\n",
    "        self.current += cur_batch_size\n",
    "\n",
    "        return synabses, pure_errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:10.236589100Z",
     "start_time": "2025-06-03T20:15:10.216581500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SquaredReLU(nn.Module):\n",
    "    def __init__(self, max_value=10.0):\n",
    "        super().__init__()\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def forward(self, x):\n",
    "        relu_output = torch.relu(x)\n",
    "        clamped_output = torch.clamp(relu_output, max=self.max_value)\n",
    "        return clamped_output ** 2\n",
    "\n",
    "\n",
    "class ElementaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ElementaryCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        if output.dim() != 2 or target.dim() != 2:\n",
    "            raise ValueError(f'Both_should_be_2D_tensors_error. Output: {output.shape}, target: {target.shape}')\n",
    "        \n",
    "        if output.shape != target.shape:\n",
    "            raise ValueError(f'Shape mismatch. Output: {output.shape}, target: {target.shape}')\n",
    "        \n",
    "        losses = torch.stack([F.cross_entropy(output[:, _], target[:, _]) for _ in range(output.size(1))])\n",
    "\n",
    "        return losses.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaQ1NZfThR9T"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:10.923015900Z",
     "start_time": "2025-06-03T20:15:10.913117800Z"
    },
    "id": "GlMBbHnKGwGb"
   },
   "outputs": [],
   "source": [
    "\n",
    "class NoiseEstimator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=11, activation=nn.ReLU, device=torch.device(\"cpu\")):\n",
    "        super(NoiseEstimator, self).__init__()\n",
    "        \n",
    "        # –í—Ö–æ–¥–Ω–æ–π. —Å–∏–Ω–¥—Ä–æ–º + –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # –°–∫—Ä—ã—Ç–Ω—ã–µ. –ø—Ä–µ–¥—ã–¥—É—â–∏–µ + –≤—Ö–æ–¥\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size + input_size, hidden_size) for _ in range(num_layers - 2)])\n",
    "\n",
    "        # –í—ã—Ö–æ–¥–Ω–æ–π. external Sigmoid\n",
    "        self.output_layer = nn.Linear(hidden_size + input_size, output_size)\n",
    "        \n",
    "        # –ê–∫—Ç–∏–≤–∞—Ü–∏—è.\n",
    "        self.activation = activation()\n",
    "\n",
    "        self.defaults = {\n",
    "            'input_size' : input_size,\n",
    "            'hidden_size' : hidden_size,\n",
    "            'output_size' : output_size,\n",
    "            'activation' : activation,\n",
    "            'num_layers' : num_layers\n",
    "        }\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.activation(self.input_layer(x))\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "          \n",
    "            y_1 = torch.cat((x, y), dim=1)\n",
    "            y_2 = layer(y_1)\n",
    "            y = self.activation(y_2)\n",
    "\n",
    "        y_1 = torch.cat((x, y), dim=1)\n",
    "        y = self.output_layer(y_1)\n",
    "        \n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:11.635826400Z",
     "start_time": "2025-06-03T20:15:11.622821Z"
    },
    "id": "8fNiTFw54o1Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_checkpoint(\n",
    "        model, opt, \n",
    "        metric_values=None, model_info=None,\n",
    "        checkpoint_dir='checkpoints', additional_mark=None, add_time=False\n",
    "    ):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_class' : model.__class__,\n",
    "        'model_config' : model.defaults,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        \n",
    "        'optimizer_class' : opt.__class__,\n",
    "        'optimizer_config' : opt.defaults,\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "        \n",
    "        'metric_values' : metric_values,\n",
    "        'model_info' : model_info,\n",
    "    }\n",
    "\n",
    "    checkpoint_model_name = str(model.__class__).split(\".\")[-1].rstrip(\"\\'>\")\n",
    "    model_code_len = checkpoint['model_config']['input_size'] - checkpoint['model_config']['output_size']\n",
    "\n",
    "    scheduler_name = 'None'\n",
    "    if checkpoint['model_info']['scheduler'] is not None:\n",
    "        scheduler_name = str(checkpoint['model_info']['scheduler'].__class__).split('.')[-1].rstrip(\"'>\") + '_'\n",
    "    \n",
    "    marker = ''\n",
    "    if additional_mark is not None:\n",
    "        marker = '_' + additional_mark + '_'\n",
    "\n",
    "    checkpoint_path = os.path.join(\n",
    "        checkpoint_dir,\n",
    "        f'{checkpoint_model_name}_'\n",
    "        f\"{str(checkpoint['model_config']['num_layers'])}_\"\n",
    "        f\"{str(checkpoint['model_config']['activation']())[:-2]}_\"\n",
    "        f\"{str(model_code_len)}_\"\n",
    "        f\"{scheduler_name}\"\n",
    "        f\"{marker}\"\n",
    "        f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S') if add_time else ''}.pth\"\n",
    "    )\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Saved checkpoint at {checkpoint_path}\")\n",
    "\n",
    "    return checkpoint_path\n",
    "\n",
    "\n",
    "def load_checkpoint(path, device=torch.device(\"cpu\")):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No such checkpoint file {path}\")\n",
    "\n",
    "    checkpoint = torch.load(path, weights_only=False)\n",
    "\n",
    "    try:\n",
    "        model  = checkpoint['model_class'](**checkpoint['model_config'], device = device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f'Can\\'t load model due to incorrect checkpoint info, bastard. {str(e)}')\n",
    "\n",
    "    try:\n",
    "        optimizer = checkpoint['optimizer_class'](model.parameters(), **checkpoint['optimizer_config'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    except:\n",
    "        raise ValueError('Can\\'t load optimizer for you due to incorrect checkpoint optimizer info, bastard')\n",
    "\n",
    "    model_info = checkpoint['model_info']\n",
    "    metric_values = checkpoint['metric_values']\n",
    "    \n",
    "    print(f\"Loaded checkpoint at {path}\")\n",
    "    return model, optimizer, model_info, metric_values\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:12.808307500Z",
     "start_time": "2025-06-03T20:15:12.793644500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class warmup_cosine_scheduler:\n",
    "    def __init__(self, optimizer, num_epochs, warmup_epochs, min_lr):\n",
    "        cosine_t_max = num_epochs - warmup_epochs\n",
    "        if cosine_t_max <= 0:\n",
    "            raise ValueError(\"T_max –¥–ª—è CosineAnnealingLR <= 0. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–µ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\")            \n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.num_epochs = num_epochs\n",
    "        self.warmup_scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.warmup_lr_scheduler)\n",
    "        self.cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.num_epochs - self.warmup_epochs, eta_min=min_lr)\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            self.warmup_scheduler.step()\n",
    "        else:\n",
    "            self.cosine_scheduler.step()\n",
    "    \n",
    "    @staticmethod\n",
    "    def warmup_lr_scheduler(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svL9DAdKhLEw"
   },
   "source": [
    "# Train and Eval cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:20.160724600Z",
     "start_time": "2025-06-03T20:15:20.138562Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score, AUROC\n",
    "# from tqdm.auto import tqdm\n",
    "if device == \"cuda\":\n",
    "    import gc\n",
    "\n",
    "\n",
    "def eval_model(\n",
    "        model, \n",
    "        eval_config={\n",
    "            'val_data_size' : 65536,\n",
    "            'batch_size' : 1024,\n",
    "            'classification_threshold' : 0.5, \n",
    "            'val_snr' : 4, \n",
    "            'early_stop_max_errors' : None # 100 errors if so\n",
    "        }, \n",
    "        device=torch.device(\"cpu\"), verbose=True\n",
    "    ):\n",
    "    \n",
    "    val_loader = InfiniteDataloader(\n",
    "            limit=eval_config['val_data_size'],\n",
    "            gen_function=gen_data,\n",
    "            device=device,\n",
    "            batch_size=eval_config['batch_size'],\n",
    "            shuffle=False,\n",
    "            snr=eval_config['val_snr']\n",
    "        )\n",
    "\n",
    "    criterion = ElementaryCrossEntropyLoss()\n",
    "    \n",
    "    metric_evaluators = {\n",
    "        'accuracy' : Accuracy(task=\"binary\").to(device),\n",
    "        'precision' : Precision(task=\"binary\").to(device),\n",
    "        'recall' : Recall(task=\"binary\").to(device),\n",
    "        'f1score' : F1Score(task=\"binary\").to(device)\n",
    "    }\n",
    "    \n",
    "    metric_values = { 'accuracy' : 0, 'precision' : 0, 'recall' : 0, 'f1score': 0}\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for metric in metric_evaluators.values():\n",
    "            metric.reset()\n",
    "\n",
    "        val_loader_iterable = val_loader\n",
    "\n",
    "        if verbose:\n",
    "            val_loader_iterable = tqdm(val_loader, desc=f\"Model [Validation]\")\n",
    "        \n",
    "        for syn_abses_batch, perrors_batch in val_loader_iterable:\n",
    "            outputs = model(syn_abses_batch)\n",
    "            total_val_loss += criterion(outputs, perrors_batch)\n",
    "            \n",
    "            # –ü–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "            outputs_probabilities = torch.sigmoid(outputs)\n",
    "            output_classes = (outputs_probabilities >= eval_config['classification_threshold']).float()\n",
    "\n",
    "            # Metrics update\n",
    "            for metric in metric_evaluators.values():\n",
    "                metric.update(output_classes, perrors_batch)\n",
    "\n",
    "            if eval_config['early_stop_max_errors'] is not None:\n",
    "                current_errors = metric_evaluators['precision'].fp + metric_evaluators['recall'].fn\n",
    "                if current_errors > eval_config['early_stop_max_errors']:\n",
    "                    break\n",
    "            \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        for metric, evaluator in metric_evaluators.items():\n",
    "            metric_values[metric] = evaluator.compute()\n",
    "\n",
    "        metric_values['loss'] = avg_val_loss\n",
    "        metric_values['correct_predictions'] = metric_evaluators['accuracy'].tp + metric_evaluators['accuracy'].tn\n",
    "        metric_values['total_predictions'] = metric_values['correct_predictions'] + metric_evaluators['accuracy'].fp + metric_evaluators['accuracy'].fn\n",
    "\n",
    "        return metric_values\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        model, optimizer, scheduler = None,\n",
    "        train_config = {\n",
    "            'num_epochs'               : 64,\n",
    "            'train_data_size'          : 65536,\n",
    "            'val_data_size'            : 32768,\n",
    "            'batch_size'               : 1024,\n",
    "            'train_snr'                : 4,\n",
    "            'val_snr'                  : 4,\n",
    "            'classification_threshold' : 0.5,\n",
    "            'start_epoch'              : 0,\n",
    "            'shuffle_data'             : False,\n",
    "            'early_stop_bad_epochs'    : None,\n",
    "            'early_stop_max_errors'    : None,\n",
    "        },\n",
    "        checkpoint_dir = 'checkpoints',\n",
    "        checkpoint_mark = None,\n",
    "        checkpoint_timestamps = True,\n",
    "        verbose = True,\n",
    "        device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "    \n",
    "    train_loader     = InfiniteDataloader(\n",
    "        limit        = train_config['train_data_size'],\n",
    "        gen_function = gen_data,\n",
    "        device       = device,\n",
    "        batch_size   = train_config['batch_size'],\n",
    "        shuffle      = train_config['shuffle_data'],\n",
    "        snr          = train_config['train_snr']\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Using device: {device}\")\n",
    "    \n",
    "    criterion = ElementaryCrossEntropyLoss()\n",
    "    \n",
    "    last_val_loss = float('inf')\n",
    "    \n",
    "    metric_evaluators = {\n",
    "        'accuracy' : Accuracy(task=\"binary\").to(device),\n",
    "        'precision' : Precision(task=\"binary\").to(device),\n",
    "        'recall' : Recall(task=\"binary\").to(device),\n",
    "        'f1score' : F1Score(task=\"binary\").to(device)\n",
    "    }\n",
    "\n",
    "    best_metric_values = {'accuracy' : -1, 'precision' : -1, 'recall' : -1, 'f1score': -1}\n",
    "    metric_values = {'accuracy' : 0, 'precision' : 0, 'recall' : 0, 'f1score': 0}\n",
    "\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    epoch_no_progress = 0\n",
    "\n",
    "    best_checkpoint_path = None\n",
    "    losses = dict()\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(train_config['start_epoch'], num_epochs):\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        train_loader_iterable = train_loader\n",
    "        if verbose:\n",
    "            train_loader_iterable = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "\n",
    "        for syn_abses_batch, perrors_batch in train_loader_iterable:            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(syn_abses_batch)\n",
    "            loss = criterion(outputs, perrors_batch)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        # –®–∞–≥ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ LR\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(epoch)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # validation\n",
    "        metric_values = eval_model(\n",
    "            model, \n",
    "            eval_config={\n",
    "                'val_data_size' : train_config['val_data_size'],\n",
    "                'batch_size' : train_config['batch_size'],\n",
    "                'classification_threshold' : train_config['classification_threshold'], \n",
    "                'val_snr' : train_config['val_snr'], \n",
    "                'early_stop_max_errors' : train_config['early_stop_max_errors'],\n",
    "            }, \n",
    "            device=device, verbose=True\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1} Results:\")\n",
    "            print((\n",
    "                f\"Loss: {metric_values['loss']:.4f} \"\n",
    "                f\"({(metric_values['loss'] / last_val_loss -1)*100:+.4f}% \"\n",
    "                f\"{'üòä' if metric_values['loss'] - last_val_loss < 0 else 'üòí'})\"\n",
    "            ))\n",
    "            print((\n",
    "                f\"Accuracy: {metric_values['accuracy']:.4f} \"\n",
    "                f\"({(metric_values['accuracy'] / best_metric_values['accuracy'] -1)*100:+.4f}% \" \n",
    "                f\"{'üëç' if metric_values['accuracy'] - best_metric_values['accuracy'] > 0 else 'üëé'})\"\n",
    "            ))\n",
    "            print(f\"Precision: {metric_values['precision']:.4f}\")\n",
    "            print(f\"Recall: {metric_values['recall']:.4f}\")\n",
    "            print(f\"F1 Score: {metric_values['f1score']:.4f}\")\n",
    "\n",
    "        losses[epoch] = metric_values['loss']\n",
    "        last_val_loss = metric_values['loss']\n",
    "        \n",
    "        if metric_values['accuracy'] <= best_metric_values['accuracy']:\n",
    "            epoch_no_progress += 1\n",
    "        else:\n",
    "            for name, value in metric_values.items():\n",
    "                best_metric_values[name] = value\n",
    "    \n",
    "            best_checkpoint_path = save_checkpoint(\n",
    "                model, \n",
    "                optimizer,\n",
    "                metric_values = metric_values,\n",
    "                model_info = {\n",
    "                    'epoch' : epoch + 1,\n",
    "                    'train_snr' : train_config['train_snr'],\n",
    "                    'val_snr' : train_config['val_snr'],\n",
    "                    'optimal_classification_threshold' : None,\n",
    "                    'scheduler' : scheduler,\n",
    "                    'batch_size' : train_config['batch_size']\n",
    "                },\n",
    "                checkpoint_dir=checkpoint_dir,\n",
    "                additional_mark = checkpoint_mark,\n",
    "                add_time=checkpoint_timestamps, \n",
    "            )\n",
    "            \n",
    "            epoch_no_progress = 0\n",
    "\n",
    "        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞\n",
    "        if (train_config['early_stop_bad_epochs'] is not None) and (epoch_no_progress >= train_config['early_stop_bad_epochs']):\n",
    "            if verbose:\n",
    "                print(f\"Early stopping triggered. {train_config['early_stop_bad_epochs']} epochs of NO progress\")\n",
    "            break\n",
    "\n",
    "    # –í—ã–≤–æ–¥ –ª—É—á—à–∏—Ö –º–µ—Ç—Ä–∏–∫ –∑–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª.\n",
    "    if verbose:\n",
    "        print(f\"\\n~~~ Loop Ended ~~~\")\n",
    "        print(f\"Best Validation Metrics Achieved:\")\n",
    "        for name in ['accuracy','loss','precision','recall','f1score']:\n",
    "            print(f\"{name}: {best_metric_values[name]:.4f}\")\n",
    "        print(f\"Total epochs: {epoch + 1}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # saving losses\n",
    "    loss_dir = os.path.join(checkpoint_dir, 'loss_stats')\n",
    "    os.makedirs(loss_dir, exist_ok=True)\n",
    "    losses_path = os.path.join(loss_dir, best_checkpoint_path.split('\\\\')[-1].split('.')[0] + '_losses.txt')\n",
    "    with open(losses_path, 'w') as loss_file:\n",
    "        for epoch, loss in losses.items():\n",
    "            loss_file.write(f'{epoch};{loss:.4f}' + '\\n')\n",
    "    \n",
    "    train_report = {\n",
    "        'path'             : best_checkpoint_path,\n",
    "        'best_val_metrics' : best_metric_values,\n",
    "        'total_epochs'     : epoch + 1\n",
    "    }\n",
    "    \n",
    "    return train_report\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:15:23.022896600Z",
     "start_time": "2025-06-03T20:15:23.016975800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def optimize_threshold(\n",
    "        model, \n",
    "        eval_config={\n",
    "            'val_data_size' : 32768,\n",
    "            'batch_size' : 1024,\n",
    "            'val_snr' : 4, \n",
    "        }, \n",
    "        device=torch.device(\"cpu\"), verbose=True\n",
    "    ):\n",
    "    val_loader = InfiniteDataloader(\n",
    "        limit=eval_config['val_data_size'],\n",
    "        gen_function=gen_data,\n",
    "        device=device,\n",
    "        batch_size=eval_config['batch_size'],\n",
    "        shuffle=False,\n",
    "        snr=eval_config['val_snr']\n",
    "    )\n",
    "\n",
    "    all_probs = []\n",
    "    all_trues = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loader_iterable = val_loader\n",
    "        if verbose:\n",
    "            val_loader_iterable = tqdm(val_loader, desc=f\"Model [Validation]\")\n",
    "        \n",
    "        for syn_abses_batch, perrors_batch in val_loader_iterable:\n",
    "            outputs = model(syn_abses_batch)\n",
    "            \n",
    "            outputs_probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "            perrors_numpy = perrors_batch.cpu().numpy()\n",
    "        \n",
    "            all_probs.append(outputs_probabilities.flatten())\n",
    "            all_trues.append(perrors_numpy.flatten())\n",
    "\n",
    "    all_probs = np.array(all_probs).flatten()\n",
    "    all_trues = np.array(all_trues).flatten()\n",
    "        \n",
    "    precs, recs, thrs = precision_recall_curve(all_trues, all_probs)\n",
    "    f1s = 2 * (precs * recs) / (precs + recs + 1e-8)\n",
    "    optimal_idx = np.argmax(f1s)\n",
    "    optimal_threshold = thrs[optimal_idx]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Optimal threshold found: {optimal_threshold:.4f}\")\n",
    "        print(f\"Max F1-score: {f1s[optimal_idx]:.4f}\")\n",
    "    \n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:16:39.383848600Z",
     "start_time": "2025-06-03T20:16:39.369312200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def checkpoints_analyze(checkpoints_dir, criterion, research_dir='metrics', additional_mark='', eval_config=None, metrics_open='w'): # or 'a'\n",
    "    best_checkpoints = defaultdict(lambda: {'accuracy' : 0 })\n",
    "    \n",
    "    for checkpoint_name in [f for f in os.listdir(checkpoints_dir) if f.endswith('pth')]:\n",
    "        checkpoint_path = os.path.join(checkpoints_dir, checkpoint_name)\n",
    "        \n",
    "        if not os.path.isfile(checkpoint_path):\n",
    "            continue\n",
    "\n",
    "        model, optim, model_info, metric_values = load_checkpoint(checkpoint_path, device)\n",
    "\n",
    "        model_config = model.defaults\n",
    "\n",
    "        if eval_config is None:\n",
    "            eval_config = {\n",
    "                'val_data_size' : 65536,\n",
    "                'batch_size' : 1024,\n",
    "                'classification_threshold' : model_info['optimal_classification_threshold'],\n",
    "                'val_snr' : model_info['val_snr'],\n",
    "                'early_stop_max_errors' : None\n",
    "            }\n",
    "\n",
    "        optimal_metrics = eval_model(model, eval_config, device)\n",
    "        print(metric_values['accuracy'], optimal_metrics['accuracy'], metric_values['accuracy'] < optimal_metrics['accuracy'])\n",
    "        if metric_values['accuracy'] < optimal_metrics['accuracy']:\n",
    "\n",
    "            metric_values = optimal_metrics\n",
    "            print('optimal threshold set')\n",
    "\n",
    "\n",
    "        model_data = {**model_info, ** model_config, **metric_values}\n",
    "\n",
    "        if metric_values['accuracy'] > best_checkpoints[model_data[criterion]]['accuracy']:\n",
    "\n",
    "            best_checkpoints[model_data[criterion]] = {\n",
    "                'accuracy' : metric_values['accuracy'],\n",
    "                'path' : checkpoint_path,\n",
    "                'model_config' : model_config,\n",
    "                'valuable_data' : model_data,\n",
    "            }\n",
    "    \n",
    "    models_sorted_by_accuracy = sorted(\n",
    "        best_checkpoints.items(),\n",
    "        key=lambda x: x[1]['accuracy'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print('\\n\\noverall results', end='\\n\\n\\n')\n",
    "    for crit, stats in best_checkpoints.items():\n",
    "        print(f'{criterion} = {crit}:', end=' ')\n",
    "        for name in ['accuracy', 'precision', 'recall', 'f1score', 'epoch', 'train_snr']:\n",
    "            print(f\"{stats['valuable_data'][name]:.4f}\", end=', ')\n",
    "        print()\n",
    "\n",
    "    research_dir = os.path.join(checkpoint_dir, research_dir)\n",
    "    os.makedirs(research_dir, exist_ok=True)\n",
    "    res_path = os.path.join(research_dir, 'metrics.txt')\n",
    "    \n",
    "    print('\\n\\n\\nsorted results', end='\\n\\n\\n')\n",
    "    with open(res_path, metrics_open) as metr_file:\n",
    "        for crit, stats in models_sorted_by_accuracy:\n",
    "            print(f'{criterion} = {crit}:', end=' ')\n",
    "            \n",
    "            metr_file.write(f'{crit};')\n",
    "            for name in ['accuracy', 'precision', 'recall', 'f1score', 'epoch', 'train_snr']:\n",
    "                metr_file.write(f\"{stats['valuable_data'][name]:.4f};\")\n",
    "                \n",
    "                print(f\"{stats['valuable_data'][name]:.4f}\", end=', ')\n",
    "            print()\n",
    "            for name in ['activation', 'scheduler']:\n",
    "                metr_file.write(f\"{str(stats['valuable_data'][name])};\")\n",
    "            metr_file.write(f'{str(additional_mark)}' + '\\n')\n",
    "\n",
    "    return models_sorted_by_accuracy, best_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:16:40.370674600Z",
     "start_time": "2025-06-03T20:16:40.336464600Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product \n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def draw_BERSNR_with_accuracy(\n",
    "        data, \n",
    "        dir='graphs', \n",
    "        label='random', \n",
    "        filename='random',\n",
    "        save_format='png', \n",
    "        dpi=300,\n",
    "        fig_size = (12, 9),\n",
    "        mark_every = 5,\n",
    "        plot_title = ''\n",
    "    ):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    \n",
    "    graph_path = os.path.join(dir, filename + f'_BERSNR.{save_format}')\n",
    "    \n",
    "    bers = [1 - stats['accuracy'].cpu() for stats in data.values()]\n",
    "    snrs = data.keys()\n",
    "    plt.figure(figsize=fig_size)\n",
    "\n",
    "\n",
    "    plt.plot(snrs, bers, marker='o', markevery=mark_every, linestyle='-', color='b', label=label)\n",
    "\n",
    "    plt.title(plot_title, fontsize=14)\n",
    "    plt.xlabel(\"SNR (dB)\", fontsize=12)\n",
    "    plt.ylabel(\"BER (log scale)\", fontsize=12)\n",
    "    plt.yscale('log', base=10)\n",
    "    plt.grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    if save_format.lower() == 'png':\n",
    "        plt.savefig(graph_path, dpi=dpi, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(graph_path, bbox_inches='tight', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_multiple_BERSNR(\n",
    "        multimodel_data, \n",
    "        dir='graphs',\n",
    "        filename='random_multimodel', \n",
    "        save_format='png', \n",
    "        dpi=300,\n",
    "        fig_size = (12, 9),\n",
    "        mark_every = 5,\n",
    "        plot_title = '' # \"BER to SNR dependency (multiple models)\"\n",
    "    ):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    graph_path = os.path.join(dir, f'{filename}_BERSNR.{save_format}')\n",
    "    \n",
    "    plt.figure(figsize=fig_size)\n",
    "    \n",
    "    # –¶–≤–µ—Ç–∞ –∏ —Å—Ç–∏–ª–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–∏—è –º–æ–¥–µ–ª–µ–π\n",
    "    colors = plt.cm.tab20.colors\n",
    "\n",
    "\n",
    "    class get_unique_marker_style():\n",
    "        def __init__(self):\n",
    "            markers = ['o', 's', '^', 'D', 'v', 'p', '*', 'h', '8', 'X']\n",
    "            styles = ['-', '--', '-.', ':']\n",
    "            self.combinations = list(product(markers, styles))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.combinations)\n",
    "\n",
    "        def __iter__(self):\n",
    "            self.current = 0\n",
    "            return self\n",
    "\n",
    "        def __next__(self):\n",
    "            if self.current >= self.__len__():\n",
    "                s = get_unique_marker_style()\n",
    "                self = iter(s)\n",
    "\n",
    "                \n",
    "            idx = random.randint(0, self.__len__() - 1)\n",
    "            \n",
    "            return self.combinations.pop(idx)\n",
    "            \n",
    "    markerstyle = iter(get_unique_marker_style())\n",
    "    \n",
    "    for idx, (model_name, data) in enumerate(multimodel_data.items()):\n",
    "        bers = [1 - stats['accuracy'].cpu() for stats in data.values()]\n",
    "        snrs = list(data.keys())\n",
    "\n",
    "        color = colors[idx % len(colors)]\n",
    "        marker, style = markerstyle.__next__()\n",
    "        \n",
    "        plt.plot(\n",
    "            snrs, bers, \n",
    "            marker=marker, \n",
    "            linestyle=style, \n",
    "            color=color, \n",
    "            label=model_name,\n",
    "            markevery=mark_every\n",
    "        )\n",
    "    \n",
    "    plt.title(plot_title, fontsize=14)\n",
    "    plt.xlabel(\"SNR (dB)\", fontsize=12)\n",
    "    plt.ylabel(\"BER (log scale)\", fontsize=12)\n",
    "    plt.yscale('log', base=10)\n",
    "    plt.grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    if save_format.lower() == 'png':\n",
    "        plt.savefig(graph_path, dpi=dpi, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(graph_path, bbox_inches='tight', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def accuracy_to_snr_checkpoint_research(\n",
    "        checkpoint_path,\n",
    "        research_config = {\n",
    "            'dir' : 'SNRs',\n",
    "            'max_size' : 1e7,\n",
    "            'batch_size' : 1024,\n",
    "            'max_errors' : 100,\n",
    "            \n",
    "            'snr_left' : 1,\n",
    "            'snr_right' : 5.5, # 5.5 for 73, 7.5 for 105\n",
    "            'snr_step' : 0.1\n",
    "        },\n",
    "        graph_config    = {\n",
    "            'draw?' : True,\n",
    "            'dir' : 'graphs', \n",
    "            'label' : 'random',\n",
    "            'filename' : 'random',\n",
    "            'save_format' : 'pdf', \n",
    "            'fig_size' : (12, 9),\n",
    "            'mark_every' : 5,\n",
    "        },\n",
    "        mark            = '',\n",
    "        verbose         = True,\n",
    "        device          = torch.device(\"cpu\")\n",
    "    ):\n",
    "\n",
    "    model, optim, model_info, metrics = load_checkpoint(checkpoint_path, device=device)\n",
    "    classification_threshold = model_info['optimal_classification_threshold']\n",
    "\n",
    "    val_res = dict()\n",
    "    \n",
    "    for cur_snr in np.arange(\n",
    "            research_config['snr_left'], \n",
    "            research_config['snr_right'] + research_config['snr_step'], \n",
    "            research_config['snr_step']\n",
    "        ):\n",
    "        val_res[cur_snr] = eval_model(\n",
    "            model,\n",
    "            eval_config={\n",
    "                'val_data_size' : research_config['max_size'],\n",
    "                'batch_size' : research_config['batch_size'],\n",
    "                'classification_threshold' : classification_threshold,\n",
    "                'val_snr' : cur_snr,\n",
    "                'early_stop_max_errors' : research_config['max_errors'] # 100 errors if so\n",
    "            },\n",
    "            device = device,\n",
    "            verbose = verbose,\n",
    "        )\n",
    "\n",
    "    checkpoint_name = checkpoint_path.split('\\\\')[-1].split('.')[0]\n",
    "    res_path = os.path.join(research_config['dir'], checkpoint_name + str(mark) + '_valres.txt')\n",
    "    \n",
    "    with open(res_path, 'w') as res_file:\n",
    "        for cur_snr, metrics in val_res.items():\n",
    "            res_file.write(\n",
    "                str(cur_snr) + ';' + ';'.join([str(val) for val in metrics.values()]) + '\\n'\n",
    "            )\n",
    "\n",
    "    if graph_config['draw?']:\n",
    "        draw_BERSNR_with_accuracy(\n",
    "            val_res,\n",
    "            dir = graph_config['dir'],\n",
    "            label = graph_config['label'],\n",
    "            filename = graph_config['filename'],\n",
    "            save_format = graph_config['save_format'],\n",
    "            fig_size = graph_config['fig_size'],\n",
    "            mark_every = graph_config['mark_every']\n",
    "        )\n",
    "\n",
    "    return val_res\n",
    "\n",
    "\n",
    "def validate_multimodal(\n",
    "        multimodel_data,\n",
    "        criterion_name,\n",
    "        checkpoints_dir='random',\n",
    "        research_config = None,\n",
    "        graph_config = None,\n",
    "        device=torch.device('cpu'),\n",
    "        verbose=False\n",
    "    ):\n",
    "    if graph_config is None:\n",
    "        graph_config = {\n",
    "            'draw?' : False,\n",
    "            'dir' : 'graphs',\n",
    "            'label' : 'random',\n",
    "            'filename' : 'random',\n",
    "            'save_format' : 'pdf',\n",
    "            'fig_size' : (12, 9),\n",
    "            'mark_every' : 5,\n",
    "        }\n",
    "    if research_config is None:\n",
    "        research_config = {\n",
    "            'dir' : 'SNRs',\n",
    "            'max_size' : 1e7,\n",
    "            'batch_size' : 1024,\n",
    "            'max_errors' : 100,\n",
    "\n",
    "            'snr_left' : 1,\n",
    "            'snr_right' : 7, # 5.5 for 73, 7.5 for 105\n",
    "            'snr_step' : 0.1\n",
    "        }\n",
    "\n",
    "    multisnr_multiaccuracy = dict()\n",
    "\n",
    "    research_config['dir'] = os.path.join(checkpoints_dir, research_config['dir'])\n",
    "    os.makedirs(research_config['dir'], exist_ok=True)\n",
    "    graph_config['dir'] = os.path.join(checkpoints_dir, graph_config['dir'])\n",
    "    os.makedirs(graph_config['dir'], exist_ok=True)\n",
    "\n",
    "    for crit, data in multimodel_data.items():\n",
    "        multisnr_multiaccuracy[f'{crit} {criterion_name}'] = accuracy_to_snr_checkpoint_research(\n",
    "            data['path'],\n",
    "            research_config=research_config,\n",
    "            graph_config=graph_config,\n",
    "            device=device,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    draw_multiple_BERSNR(\n",
    "        multisnr_multiaccuracy,\n",
    "        dir=graph_config['dir'],\n",
    "        filename=criterion_name,\n",
    "        save_format=graph_config['save_format'],\n",
    "        fig_size=graph_config['fig_size'],\n",
    "        mark_every=graph_config['mark_every']\n",
    "    )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def draw_metric_table(\n",
    "        init_data, \n",
    "        criterion, \n",
    "        numeric_column_names = [\n",
    "            'Accuracy', 'Precision', 'Recall', 'F1 Score'\n",
    "        ],\n",
    "        numeric_column_formal_names = [\n",
    "            'accuracy', 'precision', 'recall', 'f1score'\n",
    "        ],\n",
    "        categorial_column_names = [ # yea yea\n",
    "          'Epochs'  \n",
    "        ],\n",
    "        categorial_column_formal_names = [\n",
    "          'epoch'\n",
    "        ],\n",
    "        dir = 'tables',\n",
    "        table_name = 'random', \n",
    "        save_format='pdf'\n",
    "    ):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    data = [\n",
    "        [criterion, *numeric_column_names, *categorial_column_names],\n",
    "    ]\n",
    "\n",
    "    if type(init_data) == type(dict()):\n",
    "        init_data_iter = init_data.items()\n",
    "    elif type(init_data) == type(list()):\n",
    "        init_data_iter = init_data\n",
    "    \n",
    "    for name, metrics in init_data_iter:\n",
    "        table_cols = []\n",
    "        for col_name in numeric_column_formal_names:\n",
    "            table_cols.append(f\"{metrics[col_name]:.4f}\")\n",
    "        for col_name in categorial_column_formal_names:\n",
    "            table_cols.append(f\"{metrics[col_name]}\")\n",
    "        data.append([name, *table_cols])\n",
    "    # print(data)\n",
    "    fig, ax = plt.subplots(figsize=(8, 2))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=data, loc='center', cellLoc='center', colWidths=[0.2] * len(data))\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    dir = os.path.join(dir, 'tables')\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    save_path = os.path.join(dir, f\"{table_name}.{save_format}\")\n",
    "    print(save_path)\n",
    "    plt.savefig(save_path, bbox_inches='tight', format=save_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDPC 73 research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:32:56.257402300Z",
     "start_time": "2025-06-03T16:32:56.245529700Z"
    }
   },
   "source": [
    "### Number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:20:04.745336900Z",
     "start_time": "2025-06-03T20:16:50.424427700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "min_lr = 1e-6 # eta min\n",
    "initial_lr = 1e-3 #\n",
    "const_weight_decay = 1e-5\n",
    "\n",
    "optimistic_num_epochs = 96\n",
    "\n",
    "checkpoint_dir = 'optimal_layer_search_73'\n",
    "criterion = 'num_layers'\n",
    "\n",
    "search_layers = range(3, 17, 1)\n",
    "\n",
    "train_config = {\n",
    "    'num_epochs'               : optimistic_num_epochs,\n",
    "    'train_data_size'          : 64 * 1024,\n",
    "    'val_data_size'            : 32 * 1024,\n",
    "    'batch_size'               : 1024,\n",
    "    'train_snr'                : 4,\n",
    "    'val_snr'                  : 4,\n",
    "    'classification_threshold' : 0.5,\n",
    "    'start_epoch'              : 0,\n",
    "    'shuffle_data'             : False,\n",
    "    'early_stop_bad_epochs'    : 8,\n",
    "    'early_stop_max_errors'    : None,\n",
    "}\n",
    "\n",
    "\n",
    "set_seed(43)\n",
    "\n",
    "\n",
    "for num_lay in search_layers:\n",
    "    print(f\"{num_lay}-layered model train started\")\n",
    "    model = NoiseEstimator(input_size, hidden_size, output_size, num_layers=num_lay, device=device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=const_weight_decay)\n",
    "    \n",
    "    report = train_loop(\n",
    "        model, \n",
    "        optimizer, \n",
    "        scheduler = None, \n",
    "        train_config = train_config,\n",
    "        checkpoint_dir = checkpoint_dir,\n",
    "        checkpoint_timestamps = False,\n",
    "        verbose = True,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    print('classification threshold optimization')\n",
    "    \n",
    "    model, optimizer, model_info, metric_values = load_checkpoint(report['path'], device)\n",
    "    model_info['optimal_classification_threshold'] = optimize_threshold(model, device=device)\n",
    "    save_checkpoint(\n",
    "        model, \n",
    "        optimizer, \n",
    "        metric_values = metric_values, \n",
    "        model_info = model_info,\n",
    "        checkpoint_dir = checkpoint_dir,\n",
    "        add_time = False\n",
    "    )\n",
    "    \n",
    "    print(f\"{num_lay}-layered model train ended\", end='\\n\\n\\n')\n",
    "    del model\n",
    "    del optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:21:40.008005600Z",
     "start_time": "2025-06-03T20:20:04.744335800Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = 'optimal_layer_search_73'\n",
    "criterion = 'num_layers'\n",
    "top_models, all_models = checkpoints_analyze(checkpoints_dir = checkpoint_dir, criterion = criterion)\n",
    "\n",
    "universal = sorted(top_models[:3])[2] \n",
    "best_universal_layer = universal[0]\n",
    "best_warmup_epochs = universal[1]['valuable_data']['epoch'] // 2\n",
    "\n",
    "\n",
    "print('\\n\\n' + f'best {criterion} is {best_universal_layer} with ~{best_warmup_epochs * 2} epochs!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:23:50.666275500Z",
     "start_time": "2025-06-03T20:21:40.004898300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "validate_multimodal(\n",
    "    all_models,\n",
    "    criterion_name='layer',\n",
    "    checkpoints_dir=checkpoint_dir,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_metric_table({p[0] : p[1]['valuable_data'] for p in top_models}, 'Layers', dir=checkpoint_dir, table_name='layers_table_73')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T17:38:30.751589200Z",
     "start_time": "2025-06-03T17:38:17.748123Z"
    }
   },
   "source": [
    "### Activations and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:27:53.213223800Z",
     "start_time": "2025-06-03T20:26:03.574215800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "min_lr = 1e-6 # eta min\n",
    "initial_lr = 1e-3 #\n",
    "const_weight_decay = 1e-5\n",
    "warmup_epochs = 32\n",
    "\n",
    "checkpoint_dir = 'activation_lrsched_search_73'\n",
    "criterion = 'activation'\n",
    "\n",
    "activation_list = [nn.ReLU, nn.SiLU, nn.Tanh, SquaredReLU]\n",
    "scheduler_list = [None, warmup_cosine_scheduler]\n",
    "\n",
    "train_config = {\n",
    "    'num_epochs'               : optimistic_num_epochs,\n",
    "    'train_data_size'          : 64 * 1024,\n",
    "    'val_data_size'            : 32 * 1024,\n",
    "    'batch_size'               : 1024,\n",
    "    'train_snr'                : 4,\n",
    "    'val_snr'                  : 4,\n",
    "    'classification_threshold' : 0.5,\n",
    "    'start_epoch'              : 0,\n",
    "    'shuffle_data'             : False,\n",
    "    'early_stop_bad_epochs'    : 8,\n",
    "    'early_stop_max_errors'    : None,\n",
    "}\n",
    "\n",
    "\n",
    "set_seed(43)\n",
    "\n",
    "\n",
    "for activation in activation_list:\n",
    "    for scheduler_object in scheduler_list:\n",
    "        print(f\"{str(activation())[:-2]}-active model train started\")\n",
    "        \n",
    "        model = NoiseEstimator(input_size, hidden_size, output_size, \n",
    "                               num_layers=best_universal_layer, activation=activation, device=device\n",
    "                )\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=const_weight_decay)\n",
    "\n",
    "        scheduler = None\n",
    "        if scheduler_object is not None:\n",
    "            scheduler = scheduler_object(optimizer, train_config['num_epochs'], best_warmup_epochs, min_lr)\n",
    "        \n",
    "        report = train_loop(\n",
    "            model, \n",
    "            optimizer, \n",
    "            scheduler = scheduler, \n",
    "            train_config = train_config,\n",
    "            checkpoint_dir = checkpoint_dir,\n",
    "            checkpoint_timestamps = False,\n",
    "            verbose = True,\n",
    "            device = device\n",
    "        )\n",
    "    \n",
    "        print('classification threshold optimization')\n",
    "        \n",
    "        model, optimizer, model_info, metric_values = load_checkpoint(report['path'], device)\n",
    "        model_info['optimal_classification_threshold'] = optimize_threshold(model, device=device)\n",
    "        save_checkpoint(\n",
    "            model, \n",
    "            optimizer, \n",
    "            metric_values = metric_values, \n",
    "            model_info = model_info,\n",
    "            checkpoint_dir = checkpoint_dir,\n",
    "            add_time = False\n",
    "        )\n",
    "        \n",
    "        print(f\"{str(activation())[:-2]}-active model train ended\", end='\\n\\n\\n')\n",
    "        del model\n",
    "        del optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:30:04.199753700Z",
     "start_time": "2025-06-03T20:27:53.214207Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = 'activation_lrsched_search_73'\n",
    "criterion = 'activation'\n",
    "\n",
    "\n",
    "top_models = checkpoints_analyze(checkpoints_dir = checkpoint_dir, criterion = criterion)\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "best_model_data = top_models[0][0] \n",
    "best_activation = best_model_data[0]\n",
    "best_activation_name = str(best_activation())[:-2]\n",
    "act_sched = dict()\n",
    "act_sched_zip = dict()\n",
    "\n",
    "criterion = 'scheduler'\n",
    "\n",
    "for activation in activation_list:\n",
    "    act_name = str(activation())[:-2]\n",
    "    cur_dir = os.path.join(checkpoint_dir, act_name)\n",
    "    os.makedirs(cur_dir, exist_ok=True)\n",
    "    for checkpoint_name in [f for f in os.listdir(checkpoint_dir) if f.endswith('pth')]:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "        \n",
    "\n",
    "        if not '_' + act_name in checkpoint_name:\n",
    "            continue\n",
    "\n",
    "        if not os.path.isfile(checkpoint_path):\n",
    "            continue\n",
    "\n",
    "        model, optim, model_info, metric_values = load_checkpoint(checkpoint_path, device)\n",
    "\n",
    "        save_checkpoint(model, optim, metric_values = metric_values, model_info=model_info,\n",
    "                       checkpoint_dir = cur_dir)\n",
    "    \n",
    "    \n",
    "    top_models, all_models = checkpoints_analyze(checkpoints_dir = cur_dir, criterion = criterion, metrics_open='a')\n",
    "\n",
    "\n",
    "    for sch, data in top_models:\n",
    "        act_sched_zip[f'{act_name} {\"+\" if sch is not None else \"-\"}'] = data\n",
    "    \n",
    "    best_model_act_scheduler = top_models[0][0]\n",
    "    act_sched[act_name] = best_model_act_scheduler\n",
    "    print(f\"for activation {act_name} scheduler {best_model_act_scheduler}\", end='\\n\\n\\n')\n",
    "\n",
    "best_scheduler = act_sched[str(best_activation())[:-2]]\n",
    "\n",
    "print('\\n\\n' + f'best {criterion} for {best_activation_name} is {best_scheduler}!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:32:05.628610400Z",
     "start_time": "2025-06-03T20:30:04.198767100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "validate_multimodal(\n",
    "    act_sched_zip,\n",
    "    criterion_name='',\n",
    "    checkpoints_dir=checkpoint_dir,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draw_metric_table({p[0] : p[1]['valuable_data'] for p in act_sched_zip.items()}, 'Activation', dir=checkpoint_dir, table_name='activations_table_73')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:53:47.018099500Z",
     "start_time": "2025-06-03T16:51:56.080692Z"
    }
   },
   "source": [
    "### Train SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "min_lr = 1e-6 # eta min\n",
    "initial_lr = 1e-3\n",
    "const_weight_decay = 1e-5\n",
    "\n",
    "\n",
    "checkpoint_dir = 'train_snr_search_73'\n",
    "criterion = 'train_snr'\n",
    "\n",
    "train_snr_list = [1, 1.5, 2, 3, 3.4, 3.8, 4, 4.2, 4.6, 5] #\n",
    "\n",
    "train_config = {\n",
    "    'num_epochs'               : optimistic_num_epochs * 2,\n",
    "    'train_data_size'          : 64 * 1024,\n",
    "    'val_data_size'            : 32 * 1024,\n",
    "    'batch_size'               : 1024,\n",
    "    'train_snr'                : 4,\n",
    "    'val_snr'                  : 4,\n",
    "    'classification_threshold' : 0.5,\n",
    "    'start_epoch'              : 0,\n",
    "    'shuffle_data'             : False,\n",
    "    'early_stop_bad_epochs'    : 8,\n",
    "    'early_stop_max_errors'    : None,\n",
    "}\n",
    "\n",
    "\n",
    "set_seed(43)\n",
    "\n",
    "\n",
    "for train_snr in train_snr_list:\n",
    "    print(f\"{str(train_snr)}-snr driven model train started\")\n",
    "    \n",
    "    model = NoiseEstimator(input_size, hidden_size, output_size, \n",
    "                           num_layers=best_universal_layer, activation=best_activation, device=device\n",
    "            )\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=const_weight_decay)\n",
    "\n",
    "    scheduler = None\n",
    "    if best_scheduler is not None:\n",
    "        scheduler = best_scheduler.__class__(optimizer, train_config['num_epochs'], best_warmup_epochs, min_lr)\n",
    "\n",
    "    train_config['train_snr'] = train_snr\n",
    "    report = train_loop(\n",
    "        model, \n",
    "        optimizer, \n",
    "        scheduler = scheduler, \n",
    "        train_config = train_config,\n",
    "        checkpoint_dir = checkpoint_dir,\n",
    "        checkpoint_mark = f'{str(train_snr)}',\n",
    "        checkpoint_timestamps = False,\n",
    "        verbose = True,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    print('classification threshold optimization')\n",
    "    \n",
    "    model, optimizer, model_info, metric_values = load_checkpoint(report['path'], device)\n",
    "\n",
    "    model_info['optimal_classification_threshold'] = optimize_threshold(model, device=device)\n",
    "    save_checkpoint(\n",
    "        model, \n",
    "        optimizer, \n",
    "        metric_values = metric_values, \n",
    "        model_info = model_info,\n",
    "        checkpoint_dir = checkpoint_dir,\n",
    "        additional_mark = f'{str(train_snr)}',\n",
    "        add_time = False\n",
    "    )\n",
    "    \n",
    "    print(f\"{str(train_snr)}-snr driven model train ended\", end='\\n\\n\\n')\n",
    "    del model\n",
    "    del optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:36:05.348148100Z",
     "start_time": "2025-06-03T20:34:18.506131700Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = 'train_snr_search_73'\n",
    "criterion = 'train_snr'\n",
    "top_models, all_models = checkpoints_analyze(checkpoints_dir = checkpoint_dir, criterion = criterion)\n",
    "\n",
    "best_model_data = top_models[0]\n",
    "best_train_snr = best_model_data[0]\n",
    "\n",
    "print('\\n\\n' + f'best {criterion} is {best_train_snr}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:38:39.963998100Z",
     "start_time": "2025-06-03T20:36:05.345437200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "validate_multimodal(\n",
    "    all_models,\n",
    "    criterion_name='',\n",
    "    checkpoints_dir=checkpoint_dir,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_metric_table({p[0] : p[1]['valuable_data'] for p in top_models}, 'Train SNR', dir=checkpoint_dir, table_name='SNRs_table_73')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T17:52:21.968703100Z",
     "start_time": "2025-06-03T17:52:02.264445800Z"
    }
   },
   "source": [
    "# MDPC 105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T17:55:00.811049500Z",
     "start_time": "2025-06-03T17:52:21.972735100Z"
    }
   },
   "source": [
    "### Layers 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "min_lr = 1e-6 # eta min\n",
    "initial_lr = 1e-3\n",
    "const_weight_decay = 1e-5\n",
    "\n",
    "optimistic_num_epochs = 96\n",
    "\n",
    "checkpoint_dir = 'optimal_layer_search_105'\n",
    "criterion = 'num_layers'\n",
    "\n",
    "search_layers = range(3, 17, 1)\n",
    "\n",
    "train_config = {\n",
    "    'num_epochs'               : optimistic_num_epochs,\n",
    "    'train_data_size'          : 64 * 1024,\n",
    "    'val_data_size'            : 32 * 1024,\n",
    "    'batch_size'               : 1024,\n",
    "    'train_snr'                : 4,\n",
    "    'val_snr'                  : 4,\n",
    "    'classification_threshold' : 0.5,\n",
    "    'start_epoch'              : 0,\n",
    "    'shuffle_data'             : False,\n",
    "    'early_stop_bad_epochs'    : 8,\n",
    "    'early_stop_max_errors'    : None,\n",
    "}\n",
    "\n",
    "\n",
    "set_seed(43)\n",
    "\n",
    "\n",
    "for num_lay in search_layers:\n",
    "    print(f\"{num_lay}-layered model train started\")\n",
    "    model = NoiseEstimator(input_size, hidden_size, output_size, num_layers=num_lay, device=device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=const_weight_decay)\n",
    "    \n",
    "    report = train_loop(\n",
    "        model, \n",
    "        optimizer, \n",
    "        scheduler = None, \n",
    "        train_config = train_config,\n",
    "        checkpoint_dir = checkpoint_dir,\n",
    "        checkpoint_timestamps = False,\n",
    "        verbose = True,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    print('classification threshold optimization')\n",
    "    \n",
    "    model, optimizer, model_info, metric_values = load_checkpoint(report['path'], device)\n",
    "\n",
    "    model_info['optimal_classification_threshold'] = optimize_threshold(model, device=device)\n",
    "    save_checkpoint(\n",
    "        model, \n",
    "        optimizer, \n",
    "        metric_values = metric_values, \n",
    "        model_info = model_info,\n",
    "        checkpoint_dir = checkpoint_dir,\n",
    "        add_time = False\n",
    "    )\n",
    "    \n",
    "    print(f\"{num_lay}-layered model train ended\", end='\\n\\n\\n')\n",
    "    del model\n",
    "    del optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint_dir = 'optimal_layer_search_105'\n",
    "# criterion = 'num_layers'\n",
    "top_models, all_models = checkpoints_analyze(checkpoints_dir = checkpoint_dir, criterion = criterion)\n",
    "\n",
    "universal = sorted(top_models[:3])[0]\n",
    "best_universal_layer = universal[0]\n",
    "best_warmup_epochs = universal[1]['valuable_data']['epoch'] // 2\n",
    "\n",
    "\n",
    "print('\\n\\n' + f'best {criterion} is {best_universal_layer} with ~{best_warmup_epochs * 2} epochs!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_multimodal(\n",
    "    all_models,\n",
    "    criterion_name='layer',\n",
    "    checkpoints_dir=checkpoint_dir,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_metric_table({p[0] : p[1]['valuable_data'] for p in top_models}, 'Layers', dir=checkpoint_dir, table_name='layers_table_105')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations and Scheduler 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "min_lr = 1e-6 # eta min\n",
    "initial_lr = 1e-3\n",
    "const_weight_decay = 1e-5\n",
    "warmup_epochs = 32\n",
    "\n",
    "checkpoint_dir = 'activation_lrsched_search_105'\n",
    "criterion = 'activation'\n",
    "\n",
    "activation_list = [nn.ReLU, nn.SiLU, nn.Tanh, SquaredReLU]\n",
    "scheduler_list = [None, warmup_cosine_scheduler]\n",
    "\n",
    "train_config = {\n",
    "    'num_epochs'               : optimistic_num_epochs,\n",
    "    'train_data_size'          : 64 * 1024,\n",
    "    'val_data_size'            : 32 * 1024,\n",
    "    'batch_size'               : 1024,\n",
    "    'train_snr'                : 4,\n",
    "    'val_snr'                  : 4,\n",
    "    'classification_threshold' : 0.5,\n",
    "    'start_epoch'              : 0,\n",
    "    'shuffle_data'             : False,\n",
    "    'early_stop_bad_epochs'    : 8,\n",
    "    'early_stop_max_errors'    : None,\n",
    "}\n",
    "\n",
    "\n",
    "set_seed(43)\n",
    "\n",
    "\n",
    "for activation in activation_list:\n",
    "    for scheduler_object in scheduler_list:\n",
    "        print(f\"{str(activation())[:-2]}-active model train started\")\n",
    "        \n",
    "        model = NoiseEstimator(input_size, hidden_size, output_size, \n",
    "                               num_layers=best_universal_layer, activation=activation, device=device\n",
    "                )\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=const_weight_decay)\n",
    "\n",
    "        scheduler = None\n",
    "        if scheduler_object is not None:\n",
    "            scheduler = scheduler_object(optimizer, train_config['num_epochs'], best_warmup_epochs, min_lr)\n",
    "        \n",
    "        report = train_loop(\n",
    "            model, \n",
    "            optimizer, \n",
    "            scheduler = scheduler, \n",
    "            train_config = train_config,\n",
    "            checkpoint_dir = checkpoint_dir,\n",
    "            checkpoint_timestamps = False,\n",
    "            verbose = True,\n",
    "            device = device\n",
    "        )\n",
    "    \n",
    "        print('classification threshold optimization')\n",
    "        \n",
    "        model, optimizer, model_info, metric_values = load_checkpoint(report['path'], device)\n",
    "\n",
    "        model_info['optimal_classification_threshold'] = optimize_threshold(model, device=device)\n",
    "        save_checkpoint(\n",
    "            model, \n",
    "            optimizer, \n",
    "            metric_values = metric_values, \n",
    "            model_info = model_info,\n",
    "            checkpoint_dir = checkpoint_dir,\n",
    "            \n",
    "            add_time = False\n",
    "        )\n",
    "        \n",
    "        print(f\"{str(activation())[:-2]}-active model train ended\", end='\\n\\n\\n')\n",
    "        del model\n",
    "        del optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = 'activation_lrsched_search_105'\n",
    "# criterion = 'activation'\n",
    "\n",
    "top_models = checkpoints_analyze(checkpoints_dir = checkpoint_dir, criterion = criterion)\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "best_model_data = top_models[0][0]\n",
    "best_activation = best_model_data[0]\n",
    "best_activation_name = str(best_activation())[:-2]\n",
    "act_sched = dict()\n",
    "act_sched_zip = dict()\n",
    "\n",
    "criterion = 'scheduler'\n",
    "\n",
    "for activation in activation_list:\n",
    "    act_name = str(activation())[:-2]\n",
    "    cur_dir = os.path.join(checkpoint_dir, act_name)\n",
    "    os.makedirs(cur_dir, exist_ok=True)\n",
    "    for checkpoint_name in [f for f in os.listdir(checkpoint_dir) if f.endswith('pth')]:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "\n",
    "\n",
    "        if not '_' + act_name in checkpoint_name:\n",
    "            continue\n",
    "\n",
    "        if not os.path.isfile(checkpoint_path):\n",
    "            continue\n",
    "\n",
    "        model, optim, model_info, metric_values = load_checkpoint(checkpoint_path, device)\n",
    "\n",
    "        save_checkpoint(model, optim, metric_values = metric_values, model_info=model_info,\n",
    "                       checkpoint_dir = cur_dir)\n",
    "\n",
    "\n",
    "    top_models, all_models = checkpoints_analyze(checkpoints_dir = cur_dir, criterion = criterion, metrics_open='a')\n",
    "\n",
    "\n",
    "    for sch, data in top_models:\n",
    "        act_sched_zip[f'{act_name} {\"+\" if sch is not None else \"-\"}'] = data\n",
    "    \n",
    "    best_model_act_scheduler = top_models[0][0]\n",
    "    act_sched[act_name] = best_model_act_scheduler\n",
    "    print(f\"for activation {act_name} scheduler {best_model_act_scheduler}\", end='\\n\\n\\n')\n",
    "\n",
    "best_scheduler = act_sched[str(best_activation())[:-2]]\n",
    "\n",
    "print('\\n\\n' + f'best {criterion} for {best_activation_name} is {best_scheduler}!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "validate_multimodal(\n",
    "    act_sched_zip,\n",
    "    criterion_name='',\n",
    "    checkpoints_dir=checkpoint_dir,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_metric_table({p[0] : p[1]['valuable_data'] for p in act_sched_zip.items()}, 'Activation', dir=checkpoint_dir, table_name='activations_table_105')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SNR 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "min_lr = 1e-6 # eta min\n",
    "initial_lr = 1e-3\n",
    "const_weight_decay = 1e-5\n",
    "\n",
    "\n",
    "checkpoint_dir = 'train_snr_search_105'\n",
    "criterion = 'train_snr'\n",
    "\n",
    "train_snr_list = [1, 1.5, 2, 3, 3.4, 3.8, 4, 4.2, 4.6, 5] #\n",
    "\n",
    "train_config = {\n",
    "    'num_epochs'               : optimistic_num_epochs,\n",
    "    'train_data_size'          : 64 * 1024,\n",
    "    'val_data_size'            : 32 * 1024,\n",
    "    'batch_size'               : 1024,\n",
    "    'train_snr'                : 4,\n",
    "    'val_snr'                  : 4,\n",
    "    'classification_threshold' : 0.5,\n",
    "    'start_epoch'              : 0,\n",
    "    'shuffle_data'             : False,\n",
    "    'early_stop_bad_epochs'    : 8,\n",
    "    'early_stop_max_errors'    : None,\n",
    "}\n",
    "\n",
    "\n",
    "set_seed(43)\n",
    "\n",
    "\n",
    "for train_snr in train_snr_list:\n",
    "    print(f\"{str(train_snr)}-snr driven model train started\")\n",
    "    \n",
    "    model = NoiseEstimator(input_size, hidden_size, output_size, \n",
    "                           num_layers=best_universal_layer, activation=best_activation, device=device\n",
    "            )\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=const_weight_decay)\n",
    "\n",
    "    scheduler = None\n",
    "    if best_scheduler is not None:\n",
    "        scheduler = best_scheduler(optimizer, train_config['num_epochs'], best_warmup_epochs, min_lr)\n",
    "\n",
    "    train_config['train_snr'] = train_snr\n",
    "    report = train_loop(\n",
    "        model, \n",
    "        optimizer, \n",
    "        scheduler = scheduler, \n",
    "        train_config = train_config,\n",
    "        checkpoint_dir = checkpoint_dir,\n",
    "        checkpoint_mark = f'{str(train_snr)}',\n",
    "        checkpoint_timestamps = False,\n",
    "        verbose = True,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    print('classification threshold optimization')\n",
    "    \n",
    "    model, optimizer, model_info, metric_values = load_checkpoint(report['path'], device)\n",
    "\n",
    "    model_info['optimal_classification_threshold'] = optimize_threshold(model, device=device)\n",
    "    save_checkpoint(\n",
    "        model, \n",
    "        optimizer, \n",
    "        metric_values = metric_values, \n",
    "        model_info = model_info,\n",
    "        checkpoint_dir = checkpoint_dir,\n",
    "        additional_mark = f'{str(train_snr)}',\n",
    "        add_time = False\n",
    "    )\n",
    "    \n",
    "    print(f\"{str(train_snr)}-snr driven model train ended\", end='\\n\\n\\n')\n",
    "    del model\n",
    "    del optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = 'train_snr_search_105'\n",
    "# criterion = 'train_snr'\n",
    "top_models, all_models = checkpoints_analyze(checkpoints_dir = checkpoint_dir, criterion = criterion)\n",
    "\n",
    "best_model_data = top_models[0]\n",
    "best_train_snr = best_model_data[0]\n",
    "\n",
    "print('\\n\\n' + f'best {criterion} is {best_train_snr}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "validate_multimodal(\n",
    "    all_models,\n",
    "    criterion_name='',\n",
    "    checkpoints_dir=checkpoint_dir,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_metric_table({p[0] : p[1]['valuable_data'] for p in top_models}, 'Train SNR', dir=checkpoint_dir, table_name='SNRs_table_105')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
